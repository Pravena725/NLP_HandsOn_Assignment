{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Hands On 3",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAB8ghrIu3T8",
        "outputId": "36d834e9-b1c7-44c3-84dc-536f6a851718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word indexes:\n",
            "{'woof': 2, 'squeak': 1, 'meow': 0}\n",
            "\n",
            "idf values:\n",
            "meow  :  1.2876820724517808\n",
            "squeak  :  1.2876820724517808\n",
            "woof  :  1.2876820724517808\n",
            "\n",
            "tf-idf values\n",
            "  (0, 1)\t0.4472135954999579\n",
            "  (0, 2)\t0.8944271909999159\n",
            "  (1, 0)\t0.4472135954999579\n",
            "  (1, 2)\t0.8944271909999159\n",
            "  (2, 0)\t0.7071067811865476\n",
            "  (2, 1)\t0.7071067811865476\n",
            "[[0.         0.4472136  0.89442719]\n",
            " [0.4472136  0.         0.89442719]\n",
            " [0.70710678 0.70710678 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "#import\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "d0 = 'woof woof squeak'\n",
        "d1 = 'woof woof meow'\n",
        "d2 = 'meow squeak'\n",
        "\n",
        "string = [d0,d1,d2]\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "result = tfidf.fit_transform(string)\n",
        "\n",
        "#get indexing\n",
        "print('\\nWord indexes:')\n",
        "print(tfidf.vocabulary_)\n",
        "\n",
        "#get idf values\n",
        "print('\\nidf values:')\n",
        "for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n",
        "  print(ele1,' : ', ele2)\n",
        "\n",
        "print('\\ntf-idf values')\n",
        "print(result)\n",
        "\n",
        "print(result.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfyxUhbAyGiR",
        "outputId": "df4cc830-431e-4ce1-cb88-af341a9c80ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "\n",
        "nltk.download(\"brown\")\n",
        "\n",
        "document = brown.sents()\n",
        "data = []\n",
        "for sent in document:\n",
        "  new_sent = []\n",
        "  for word in sent:\n",
        "    new_word = word.lower()\n",
        "    if new_word[0] not in string.punctuation:\n",
        "      new_sent.append(new_word)\n",
        "  if len(new_sent) > 0:\n",
        "    data.append(new_sent)\n",
        "\n",
        "\n",
        "#Creating Word2Vec\n",
        "model = Word2Vec(\n",
        "    sentences = data, \n",
        "    size = 50, \n",
        "    window = 10, \n",
        "    iter = 50,\n",
        "    )\n",
        "\n",
        "\n",
        "#Vector for word love\n",
        "print(\"Vecotr for car:\")\n",
        "print(model.wv[\"car\"])\n",
        "print()\n",
        "\n",
        "\n",
        "#Finding most similar words\n",
        "print(\"5 words similar to country : \")\n",
        "words = model.wv.most_similar(\"country\", topn=5)\n",
        "for word in words:\n",
        "  print(word)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "5d4SlegLwedT",
        "outputId": "68cfb69d-4e6c-46e4-f184-4e1f18583f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-4fdfc454b36e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing Data\n",
        "words = \"france\", \"germany\", \"India\", \"truck\", \"boat\", \"road\", \"teacher\", \"student\"]\n",
        "\n",
        "X=model.wv[words]\n",
        "pca = PCA(n_components = 2)\n",
        "result = pca.fit_transform(X)\n",
        "\n",
        "pyplot.scatter(result[:,0], result[:,1])\n",
        "for i, word in \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xq749gnLyhUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Named Entity Recognition using Spacy"
      ],
      "metadata": {
        "id": "0N-dNkl5zhjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def show_entities(doc):\n",
        "  if doc.ents:\n",
        "    for ent in doc.ents:\n",
        "      print(ent.text + '-' + ent.label_ + '-' + str(spacy.explain(ent.label_)))\n",
        "  else:\n",
        "    print(\"No entities found\")"
      ],
      "metadata": {
        "id": "F4aZMUTVzk-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'')"
      ],
      "metadata": {
        "id": "E3nEjRch0WGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'Tesla')"
      ],
      "metadata": {
        "id": "rMzCFqen0d_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding your NER to a SPAN\n",
        "\n",
        "from spacy.tokens import Span \n",
        "ORG = doc.vocab.strings[u\"ORG\"]\n",
        "print(ORG)\n",
        "\n",
        "#creating a span for new entity\n",
        "new_entity = Span(doc, 0, 1, label = ORG)\n",
        "doc.ents = list(doc.ents) + [new_entity]\n",
        "\n",
        "show_entities(doc)"
      ],
      "metadata": {
        "id": "TSVqdZRF0il8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding named entities to all matching spans\n",
        "doc = nlp(u\"Our company created a brand new \")\n",
        "\n",
        "show_entities(doc)\n",
        "\n",
        "#adding vaccum cleaner and vaccum-cleaner as NER\n",
        "from spacy.matcher import PhraseMatcher\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "phrase_list = ['vacuum cleaner', 'vacuum-cleaner']\n",
        "\n",
        "matcher.add\n"
      ],
      "metadata": {
        "id": "Av0G-Yi91ESK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating spans from these matches in making NER\n",
        "\n",
        "from spacy.tokens import span\n",
        "PROD = doc.vocab.strings[n\"PRODUCT\"]\n",
        "\n",
        "new_entities = [Span(doc,match[1], )]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5GdemXVn1t09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing NER\n",
        "\n",
        "from spacy import displacy\n",
        "sentence = \"\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "displacy.render(doc, style=\"ent\", jupyter = True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QRDU5Iht2FdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to apply color\n",
        "colors = {\"NORP\", }"
      ],
      "metadata": {
        "id": "Xo8C5-092ezS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vjcaEqoO2dm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dependency Parsing"
      ],
      "metadata": {
        "id": "baulx-WL2otR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import  displacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "sentence = \"The quick brown fox jumping over the lazy dog\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "print(f\"{'Node (from)-->':<15} {'Relation':^10} {'-->Node (to)':>15}\\n\")\n",
        "\n",
        "for token in doc:\n",
        "  \n",
        "          "
      ],
      "metadata": {
        "id": "lhG2D0dK2rE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Summarization using Spacy"
      ],
      "metadata": {
        "id": "N8rRqOmQ3-eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS"
      ],
      "metadata": {
        "id": "bZ-PmuRP4BlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = list(STOP_WORDS)\n",
        "from string import punctuation\n",
        "punctuation = punctuation + '\\n'"
      ],
      "metadata": {
        "id": "vT0EHl4u4KGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\""
      ],
      "metadata": {
        "id": "ZUg2RHMP4WgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\" The human coronavirus"
      ],
      "metadata": {
        "id": "aR7g52_14Ym9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load()"
      ],
      "metadata": {
        "id": "rWSCWClX4dqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XjlRaID24hdq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}