{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Hands - On Unit 1",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Session 1 - NLTK**"
      ],
      "metadata": {
        "id": "4JY3-ipEUrvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "MdS2ToKXUwPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9saNKsiV4eH",
        "outputId": "db2a044c-939e-4141-8454-0c70e23ce8a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U nltk\n",
        "!pip install python-Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "SI3kzaEAWStc",
        "outputId": "cbac9357-429f-4985-8c05-2f3c4f862b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2022.1.18-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
            "\u001b[K     |████████████████████████████████| 748 kB 56.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.62.3)\n",
            "Installing collected packages: regex, nltk\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.7 regex-2022.1.18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk",
                  "regex"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▌                         | 10 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20 kB 27.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 30 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 40 kB 35.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 50 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149854 sha256=a5555855a07ccd11734ed4c48f452d42cd767fc8261f69cdd59d9b0d9536a435\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statement 1** - Given a paragraph, tokenize the paragraph into seperate sentences and further tokenize the sentences into words using NLTK."
      ],
      "metadata": {
        "id": "qJQYT6rNYgo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Good morning. Welcome to today's class. I am studying Natural Language Processing.\"\n",
        "\n",
        "l1 = sent_tokenize(text)\n",
        "print(l1)\n",
        "\n",
        "l2 = word_tokenize(text)\n",
        "print(l2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2w-KsFvWpky",
        "outputId": "a1b13bea-0b11-4ca8-ac2a-3d898cf1911a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good morning.', \"Welcome to today's class.\", 'I am studying Natural Language Processing.']\n",
            "['Good', 'morning', '.', 'Welcome', 'to', 'today', \"'s\", 'class', '.', 'I', 'am', 'studying', 'Natural', 'Language', 'Processing', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statement 2** - Given a sentence use porter stemmer and snowball stemmer to obtain the stemmed form of the words in the sentence."
      ],
      "metadata": {
        "id": "OZqV2r1LZvx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "words = ['run', 'ran', 'running', 'generous', 'generate', 'generously', 'generation', 'cared', 'university', 'fairly', 'easily', 'singing','sings', 'sung', 'singer', 'sportingly']\n",
        "\n",
        "for i in words:\n",
        "  print(i, \" - \", ps.stem(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZGFg_46aKN5",
        "outputId": "6ca75e4a-81eb-4c09-bd04-6149605b5f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run  -  run\n",
            "ran  -  ran\n",
            "running  -  run\n",
            "generous  -  gener\n",
            "generate  -  gener\n",
            "generously  -  gener\n",
            "generation  -  gener\n",
            "cared  -  care\n",
            "university  -  univers\n",
            "fairly  -  fairli\n",
            "easily  -  easili\n",
            "singing  -  sing\n",
            "sings  -  sing\n",
            "sung  -  sung\n",
            "singer  -  singer\n",
            "sportingly  -  sportingli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using snowball stemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "snowball = SnowballStemmer(language = 'english')\n",
        "\n",
        "words = ['run', 'ran', 'running', 'generous', 'generate', 'generously', 'generation', 'cared', 'university', 'fairly', 'easily', 'singing','sings', 'sung', 'singer', 'sportingly']\n",
        "\n",
        "for i in words:\n",
        "  print(i, \" - \", snowball.stem(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecJNZZtfbjmf",
        "outputId": "50df6c01-5fab-4bb0-f922-ff67bb979316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run  -  run\n",
            "ran  -  ran\n",
            "running  -  run\n",
            "generous  -  generous\n",
            "generate  -  generat\n",
            "generously  -  generous\n",
            "generation  -  generat\n",
            "cared  -  care\n",
            "university  -  univers\n",
            "fairly  -  fair\n",
            "easily  -  easili\n",
            "singing  -  sing\n",
            "sings  -  sing\n",
            "sung  -  sung\n",
            "singer  -  singer\n",
            "sportingly  -  sport\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statement 3** - Using the WordNet Lemmatizer of NLTK, perform lemmatization on the given words."
      ],
      "metadata": {
        "id": "6sLnoeXEcLHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks - \",lemmatizer.lemmatize(\"rocks\", pos = 'n')) #n is for noun\n",
        "print(\"corpora - \",lemmatizer.lemmatize(\"corpora\"))\n",
        "\n",
        "print(\"looking - \",lemmatizer.lemmatize(\"looking\", pos = 'v')) #v is for verb\n",
        "\n",
        "print(\"better - \",lemmatizer.lemmatize(\"better\", pos = 'a')) #a is for adjective\n",
        "print(\"worst - \",lemmatizer.lemmatize(\"worst\", pos = 'a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7784hDcIcbRl",
        "outputId": "86cae5a9-49d4-4834-b9eb-a81a0e159b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks -  rock\n",
            "corpora -  corpus\n",
            "looking -  look\n",
            "better -  good\n",
            "worst -  bad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statement 4** - Using the NLTK library, remove all the stopwords from a given sentence."
      ],
      "metadata": {
        "id": "wf22sZxGcdfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhF14fkQcn4_",
        "outputId": "89bc23c3-80f6-48ca-8bc6-1ac00bc80336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "sent = \"\"\"One of the major forms of pre-processing is to filter out useless data. In NLP, useless data, are referred to as stopwords.\"\"\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(sent)\n",
        "\n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "  if w not in stop_words:\n",
        "    filtered_sentence.append(w)\n",
        "\n",
        "print(\"Word tokens - \", word_tokens)\n",
        "print(\"Word tokens with stop words removed - \", filtered_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv_sjXteer5h",
        "outputId": "b6c1cd30-1781-4e0f-b0e2-cd5be291db85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokens -  ['One', 'of', 'the', 'major', 'forms', 'of', 'pre-processing', 'is', 'to', 'filter', 'out', 'useless', 'data', '.', 'In', 'NLP', ',', 'useless', 'data', ',', 'are', 'referred', 'to', 'as', 'stopwords', '.']\n",
            "Word tokens with stop words removed -  ['One', 'major', 'forms', 'pre-processing', 'filter', 'useless', 'data', '.', 'In', 'NLP', ',', 'useless', 'data', ',', 'referred', 'stopwords', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statement 5** - Remove the punctuations from a given sentence     \n",
        "a) By defining manually, the list of the punctuations.     \n",
        "b) By using regular expressions."
      ],
      "metadata": {
        "id": "e5I38k3qen9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#manually\n",
        "punctuations = '''!()-{}[];:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "input_string = \"Python, a famous programming language is the easiest to learn!!\"\n",
        "\n",
        "no_punc = \"\"\n",
        "\n",
        "for char in input_string:\n",
        "  if char not in punctuations:\n",
        "    no_punc = no_punc + char\n",
        "\n",
        "print(\"Punctuation free string -: \", no_punc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prdJGAdggD8X",
        "outputId": "c5258b1f-e321-4cf2-c044-ce68a8326640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation free string -:  Python a famous programming language is the easiest to learn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#By using regular expressions\n",
        "\n",
        "import re\n",
        "\n",
        "my_string = \"Python \\is th%e bes+*t programmin!g lang#,uage..!\"\n",
        "\n",
        "op_string = re.sub(r'[^\\w\\s]','', my_string)\n",
        "\n",
        "print(\"String with punctuation - \", my_string)\n",
        "print(\"String without punctuation - \", op_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVxrDUKcgHwK",
        "outputId": "2bd3be7c-37d5-4d9d-a2cc-8ad2249b61d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "String with punctuation -  Python \\is th%e bes+*t programmin!g lang#,uage..!\n",
            "String without punctuation -  Python is the best programming language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statement 6** - TEXT PRE-PROCESSING - GIven a sentence, lower the case of the words, remove stop words and punctuations.\n",
        "\n",
        "Then print the sentence."
      ],
      "metadata": {
        "id": "Vqsv0T7PhQl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re \n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def preprocess(sentence):\n",
        "  sentence = sentence.lower()\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokens = tokenizer.tokenize(sentence)\n",
        "  filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
        "  return \" \".join(filtered_words)\n",
        "\n",
        "sentence = \"This program will lower the case! It's the program which removes all punctuations, + all stops words!\"\n",
        "print(preprocess(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aKfXlaJhhi_",
        "outputId": "f5032d77-e98f-4ae3-bb6f-09b2e5ec8930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "program lower case program removes punctuations stops words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statement 7** - Find the min edit distancereq to convert str1 to str2 using\n",
        "\n",
        "a) Recursive approach\n",
        "\n",
        "b)DP based approach"
      ],
      "metadata": {
        "id": "uCc8GW3hirU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#recursive approach\n",
        "def editDistance(str1, str2, m, n):\n",
        "  if m==0:\n",
        "    return n\n",
        "\n",
        "  if n==0:\n",
        "    return m\n",
        "\n",
        "  if str1[m-1]==str2[n-1]:\n",
        "    return editDistance(str1, str2, m-1, n-1)\n",
        "\n",
        "  return 1 + min(editDistance(str1, str2, m, n-1), editDistance(str1, str2, m-1, n), editDistance(str1, str2, m-1, n-1))\n",
        "\n",
        "str1 = \"arrive\"\n",
        "str2 = \"drive\"\n",
        "\n",
        "print(editDistance(str1, str2, len(str1), len(str2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUYm99b5i-Gy",
        "outputId": "d086ded7-6f85-4522-e43c-9cc2b5b96d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dynamic programming\n",
        "def EditDistDP(str1, str2):\n",
        "  len1 = len(str1)\n",
        "  len2 = len(str2)\n",
        "\n",
        "  DP = [[0 for i in range(len1+1)] for j in range(2)]\n",
        "\n",
        "  for i in range(0, len1+1):\n",
        "    DP[0][i] = i\n",
        "\n",
        "  for i in range(1, len2+1):\n",
        "    for j in range(0, len1+1):\n",
        "      if(j==0):\n",
        "        DP[i%2][j] = i\n",
        "\n",
        "      elif (str1[j-1] == str2[i-1]):\n",
        "        DP[i%2][j] = DP[(i-1)%2][j-1]\n",
        "\n",
        "      else:\n",
        "        DP[i%2][j] = (1 + min( DP[(i-1)%2][j], min( DP[i%2][j-1], DP[(i-1)%2][j-1])))\n",
        "\n",
        "  print(DP[len2 % 2][len1], \"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  str1 = \"abcdef\"\n",
        "  str2 = \"azcea\"\n",
        "\n",
        "EditDistDP(str1, str2)  \n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FICmT4CCj6u-",
        "outputId": "0f17a244-b641-4ede-d136-d00843663e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statement 8** - Calculate Levenshtein distance to convert str1 to str2."
      ],
      "metadata": {
        "id": "cpfVvLtlj7Nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Levenshtein import distance as lev\n",
        "\n",
        "print(lev('party', 'park'))\n",
        "print(lev('arrive', 'drive'))\n",
        "print(lev('abcdef', 'azcea'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCP5mVKXkFJk",
        "outputId": "7ac53329-7eb7-407d-894f-bcb1dd56ac26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "2\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statement 9** - Working with re libraray of python for regular expressions"
      ],
      "metadata": {
        "id": "4iiv3oA-kFdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re \n",
        "\n",
        "txt = \"Use of python in Machine Learning\"\n",
        "x = re.findall(\"in\", txt)\n",
        "print(\"1 - \", x)\n",
        "\n",
        "txt = \"Python is one of the most popular languages around the world\"\n",
        "searchObj = re.search(\"\\s\", txt)\n",
        "print(\"2 - The first white space character is located in position \", searchObj.start())\n",
        "\n",
        "txt = \"Python is one of the most popular languages around the world\"\n",
        "searchObj = re.split(\"\\s\", txt)\n",
        "print(\"3 - \", searchObj)\n",
        "\n",
        "txt = \"Python is one of the most popular languages around the world\"\n",
        "searchObj = re.sub(\"\\s\", \"_\", txt)\n",
        "print(\"4 - \", searchObj)\n",
        "\n",
        "string = \"Python is one of the most popular languages around the world\"\n",
        "searchObj = re.search(r\"\\bP\\w+\", string)\n",
        "print(\"5 - \", searchObj)\n",
        "\n",
        "txt = \"The rain in Spain\"\n",
        "x = re.findall(\"[a-m]\", txt)\n",
        "print(\"6 - \", x)\n",
        "\n",
        "txt = \"hello world\"\n",
        "x = re.findall(\"he.*o\", txt)\n",
        "print(\"7 - \", x)\n",
        "\n",
        "txt = \"hello planet\"\n",
        "x = re.findall(\"he.*o\", txt)\n",
        "print(\"8 - \", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUpZYg9RkVy6",
        "outputId": "b0c92fee-63e0-4ecd-d340-d4d95ec4a572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 -  ['in', 'in', 'in']\n",
            "2 - The first white space character is located in position  6\n",
            "3 -  ['Python', 'is', 'one', 'of', 'the', 'most', 'popular', 'languages', 'around', 'the', 'world']\n",
            "4 -  Python_is_one_of_the_most_popular_languages_around_the_world\n",
            "5 -  <re.Match object; span=(0, 6), match='Python'>\n",
            "6 -  ['h', 'e', 'a', 'i', 'i', 'a', 'i']\n",
            "7 -  ['hello wo']\n",
            "8 -  ['hello']\n"
          ]
        }
      ]
    }
  ]
}